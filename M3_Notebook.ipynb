{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38283226",
   "metadata": {},
   "source": [
    "# Notebook for M3 assesment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b5d54b3",
   "metadata": {},
   "source": [
    "This is a mock-up notebook for M3 deliverable for AIRGo project. The objective of the notebook is to showcase the end-to-end AIRGo pipeline that will be implemented in the project, illustrated on the sandbox environment provided by Grid2Op. Where relevant, mentions to how the pipeline will be different for AIRgo are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764d929",
   "metadata": {},
   "source": [
    "### Import of library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "from grid2op.PlotGrid import PlotMatplot\n",
    "from grid2op.Backend.PandaPowerBackend import PandaPowerBackend\n",
    "from grid2op.Agent import DoNothingAgent\n",
    "from grid2op.Episode import EpisodeData\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from grid2op.gym_compat import GymEnv\n",
    "from gym import Env\n",
    "from gym.utils.env_checker import check_env\n",
    "import tqdm\n",
    "from grid2op.Runner import Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbaff62",
   "metadata": {},
   "source": [
    "### Create a Grid2op environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7d71cd3",
   "metadata": {},
   "source": [
    "Here we load the l2rpn_case14_sandbox file, in the context of our project it should be France network as a whole for the final demonstration.\n",
    "\n",
    "As you can see for the experience to be reproducible we can set a seed so the train/val/test sets are always the same. \n",
    "\n",
    "<strong>The backend will be changed from PandaPowerBackend to PypowsyblBackend</strong> that is the cornerstone of the AIRGo project.\n",
    "\n",
    "The make function is highly customizable and a lot of parameters could be changed as well other classes.\n",
    "For more details: https://grid2op.readthedocs.io/en/latest/makeenv.html#grid2op.MakeEnv.make "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grid2op.make(\"l2rpn_case14_sandbox\",backend = PandaPowerBackend()) \n",
    "max_iter = 5  # we limit the number of iterations to reduce computation time. Put -1 if you don't want to limit it\n",
    "env.seed(42)\n",
    "obs = env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fcf5e9e",
   "metadata": {},
   "source": [
    "To create your train, val and test environment. Only needs to be run once !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2659822",
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    nm_env_train, nm_env_val, nm_env_test = env.train_val_split_random(pct_val=1., pct_test=1., add_for_test=\"test\")\n",
    "except Exception as e:\n",
    "    print(\"Train repositories already exist next to \"+ str(env._init_env_path)+\"! If you want so, you can delete val, train and test ones.\")\n",
    "    nm_env_train = str(env._init_env_path + \"_train\")\n",
    "    nm_env_val = str(env._init_env_path + \"_val\")\n",
    "    nm_env_test = str(env._init_env_path + \"_test\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe46a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = grid2op.make(\"l2rpn_case14_sandbox_train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6571e425",
   "metadata": {},
   "source": [
    "### Then, we can visualize our network and the data associated with each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_helper = PlotMatplot(train_env.observation_space)\n",
    "_ = plot_helper.plot_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d36b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_helper.plot_obs(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e879a4",
   "metadata": {},
   "source": [
    "### Different type of actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6af7214",
   "metadata": {},
   "source": [
    "<strong>There are five main types of possible actions</strong>:\n",
    "* Injection actions\n",
    "* Connection/Deconnection of a line\n",
    "* Topological configuration at every substation  \n",
    "\n",
    "     <em>If the correct parameters are given</em>\n",
    "* Redispatching\n",
    "* Curtailment\n",
    "\n",
    "\n",
    "\n",
    "For more details: https://grid2op.readthedocs.io/en/latest/action.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071296b",
   "metadata": {},
   "source": [
    "### Create an agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "640f1e3a",
   "metadata": {},
   "source": [
    "An agent corresponds to an algorithm aimed at taking some actions (i.e. the actions mentioned above), regarding some observations on the grid and the possible rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf3cd3b6",
   "metadata": {},
   "source": [
    "For the purpose of this illustration we chose the DoNothingAgent which takes no action at any time step of the simulation. This agent is already pre-implemented. It is also possible to create one by following the Grid2op framework instructions and rules.\n",
    "\n",
    "For further information: https://grid2op.readthedocs.io/en/latest/agent.html\n",
    "\n",
    "In the final demonstration, an RL agent will be trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7122c5f",
   "metadata": {},
   "source": [
    "This DoNothingAgent might be replaced by your personnal RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = DoNothingAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3eee9",
   "metadata": {},
   "source": [
    "### Train an agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "934eb7fe",
   "metadata": {},
   "source": [
    "We use the training environment to carry out the training phase on the model. \n",
    "It is also possible to use a complete gym environment.  \n",
    "\n",
    "For more details: https://grid2op.readthedocs.io/en/latest/gym.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env = GymEnv(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2d966ab",
   "metadata": {},
   "source": [
    "Every possible action can be listed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628aa5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "549cc3b1",
   "metadata": {},
   "source": [
    "As well as possible observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed378631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gym_env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c68461d",
   "metadata": {},
   "source": [
    "These can be modified to fit a more classical form of reinforcment learning algorithms that deal with discrete action space by using:\n",
    "\n",
    "```python\n",
    "from grid2op.gym_compat import DiscreteActSpace\n",
    "gym_env.action_space = DiscreteActSpace(training_env.action_space,\n",
    "                                        attr_to_keep=[\"set_bus\" , \"set_line_status_simple\"])\n",
    "```  \n",
    "and  \n",
    "```python\n",
    "from grid2op.gym_compat import BoxGymObsSpace\n",
    "gym_env.observation_space = BoxGymObsSpace(training_env.observation_space,\n",
    "                                           attr_to_keep=[\"rho\"])\n",
    "gym_env.observation_space\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95a5e665",
   "metadata": {},
   "source": [
    "Because our agent 'DoNothingAgent' cannot be trained, hereafter is an example of how this could be done with a neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a095ffe7",
   "metadata": {},
   "source": [
    "Once you have your trainable agent, you can run some learning iterations by using: \n",
    "```python\n",
    "from YOUR_PACKAGE import YOUR_MODEL\n",
    "nn_model = YOUR_MODEL(env=gym_env,\n",
    "               learning_rate=1e-3,\n",
    "               policy=\"YOUR_POLICY\",\n",
    "               policy_kwargs={\"net_arch\": [100, 100, 100]}, # Just an example of architecture\n",
    "               n_steps=2,\n",
    "               batch_size=8,\n",
    "               verbose=True,\n",
    "               )\n",
    "```  \n",
    "and\n",
    "```python\n",
    "nn_model.learn(total_timesteps=LEARNING_ITERATION)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea5299",
   "metadata": {},
   "source": [
    "### Evaluate your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f21a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"saved_agent_DoNothingAgent\"\n",
    "path_save_results = \"{}_results\".format(save_path)\n",
    "shutil.rmtree(path_save_results, ignore_errors=True)\n",
    "\n",
    "\n",
    "runner = Runner(**env.get_params_for_runner(),\n",
    "                agentClass=my_agent\n",
    "               )\n",
    "res = runner.run(nb_episode=1, \n",
    "                 max_iter=max_iter,\n",
    "#                  pbar=tqdm,\n",
    "                 path_save=f\"./{path_save_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The results for DoNothing agent are:\")\n",
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a818cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path_save_results)\n",
    "EpisodeData.list_episode(path_save_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episodes = EpisodeData.list_episode(path_save_results)\n",
    "this_episode = EpisodeData.from_disk(*all_episodes[0])\n",
    "li_actions = this_episode.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a38328",
   "metadata": {},
   "source": [
    "Extraction of all the actions taken by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362aa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in li_actions:\n",
    "    dict_act_ = act.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_act_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbc21815",
   "metadata": {},
   "source": [
    "As you can see, the dictionnary containing these actions is empty, which is rather logical since the agent does not take any action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df55dc1b",
   "metadata": {},
   "source": [
    "We can now check certain observation values for the episode. Typically the state of the lines (connected/disconnected) at each stage or the number of actual disconnections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_observations = this_episode.observations\n",
    "nb_real_disc = 0\n",
    "for obs_ in li_observations:\n",
    "    nb_real_disc += (obs_.line_status == False).sum()\n",
    "print(f'Total number of disconnected powerlines cumulated over all the timesteps : {nb_real_disc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5594d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_count = {}\n",
    "for act in li_actions:\n",
    "    act_as_vect = tuple(act.to_vect())\n",
    "    if not act_as_vect in actions_count:\n",
    "        actions_count[act_as_vect] = 0\n",
    "    actions_count[act_as_vect] += 1\n",
    "print(\"The agent did {} different valid actions:\\n\".format(len(actions_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52989ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in li_actions:\n",
    "    print(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e8278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ef4a4aa3d7d941a7258dcfbd0262b9d02ac8bdcd27a62f62dc81d282b1bdd78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
