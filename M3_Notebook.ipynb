{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38283226",
   "metadata": {},
   "source": [
    "# Notebook for M3 assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d54b3",
   "metadata": {},
   "source": [
    "This is a mock-up notebook for M3 deliverable for AIRGo project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764d929",
   "metadata": {},
   "source": [
    "### Import of library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "from grid2op.PlotGrid import PlotMatplot\n",
    "from grid2op.Backend.PandaPowerBackend import PandaPowerBackend\n",
    "from grid2op.Agent import DoNothingAgent\n",
    "from grid2op.Episode import EpisodeData\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from grid2op.gym_compat import GymEnv\n",
    "from gym import Env\n",
    "from gym.utils.env_checker import check_env\n",
    "import tqdm\n",
    "from grid2op.Runner import Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbaff62",
   "metadata": {},
   "source": [
    "### Create a Grid2op environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d71cd3",
   "metadata": {},
   "source": [
    "Here we load the rte_case14_realistic file, in the context of our project it should be france network as a whole for the final test.  \n",
    "\n",
    "As you can see for the experience to be reproducible we can set a seed so the train/val/test sets are always the same. \n",
    "\n",
    "The backend would be changed to PypowsyblBackend.\n",
    "\n",
    "The make function is highly customizable and a lot of parameters could be changed as well other classes.\n",
    "For more details : https://grid2op.readthedocs.io/en/latest/makeenv.html#grid2op.MakeEnv.make "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grid2op.make(\"l2rpn_case14_sandbox\",backend = PandaPowerBackend()) \n",
    "max_iter = 5  # we limit the number of iterations to reduce computation time. Put -1 if you don't want to limit it\n",
    "env.seed(42)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf5e9e",
   "metadata": {},
   "source": [
    "To create your train, val and test environment. ! Should be only runned once !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2659822",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_env_train, nm_env_val, nm_env_test = env.train_val_split_random(pct_val=1., pct_test=1.,add_for_test=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe46a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = grid2op.make(\"l2rpn_case14_sandbox_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571e425",
   "metadata": {},
   "source": [
    "### We can then visualize our network and the data associated with each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_helper = PlotMatplot(train_env.observation_space)\n",
    "_ = plot_helper.plot_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d36b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_helper.plot_obs(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e879a4",
   "metadata": {},
   "source": [
    "### Different type of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af7214",
   "metadata": {},
   "source": [
    "<strong>There is five main types of actions possible</strong> :\n",
    "* Injection actions\n",
    "* Connection/Deconnection of a line\n",
    "* Topological configuration at every substation  \n",
    "\n",
    "     <em>If the rights parameters are given</em>\n",
    "* Redispatching\n",
    "* Curtailment\n",
    "\n",
    "\n",
    "\n",
    "For more detail : https://grid2op.readthedocs.io/en/latest/action.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071296b",
   "metadata": {},
   "source": [
    "### Create an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f1e3a",
   "metadata": {},
   "source": [
    "An agent would be the algorithm that is gonna take some actions (all the possible one written just a cell above), regarding some observation on the grid and the possible rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cd3b6",
   "metadata": {},
   "source": [
    "In our case we chose the DoNothingAgent that is not gonna take any action at any time step of the simulation which is already pre-implemented. Otherwise it is possible to create one following Grid2op framework and rules.\n",
    "\n",
    "For more informations : https://grid2op.readthedocs.io/en/latest/agent.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7122c5f",
   "metadata": {},
   "source": [
    "This agent should be replaced with your personnal RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = DoNothingAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3eee9",
   "metadata": {},
   "source": [
    "### Train an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eb7fe",
   "metadata": {},
   "source": [
    "We are using train environment to achieve training phase on the model.\n",
    "\n",
    "It is also possible to use a complete gym environment.  \n",
    "For more detail : https://grid2op.readthedocs.io/en/latest/gym.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env = GymEnv(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d966ab",
   "metadata": {},
   "source": [
    "We can see all the possible actions that can be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628aa5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cc3b1",
   "metadata": {},
   "source": [
    "And the possible observations also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed378631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gym_env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68461d",
   "metadata": {},
   "source": [
    "Those can be changed to fit within a more classical form of reinforcment learning algorithms that are dealing with discrete action space using \n",
    "\n",
    "```python\n",
    "from grid2op.gym_compat import DiscreteActSpace\n",
    "gym_env.action_space = DiscreteActSpace(training_env.action_space,\n",
    "                                        attr_to_keep=[\"set_bus\" , \"set_line_status_simple\"])\n",
    "```  \n",
    "and  \n",
    "```python\n",
    "from grid2op.gym_compat import BoxGymObsSpace\n",
    "gym_env.observation_space = BoxGymObsSpace(training_env.observation_space,\n",
    "                                           attr_to_keep=[\"rho\"])\n",
    "gym_env.observation_space\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5e665",
   "metadata": {},
   "source": [
    "Because we our agent \"DoNothingAgent\" can't be trained I show an example of it could be with a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095ffe7",
   "metadata": {},
   "source": [
    "Once you have your one agent you can run some learning iterations using : \n",
    "```python\n",
    "from YOUR_PACKAGE import YOUR_MODEL\n",
    "nn_model = YOUR_MODEL(env=gym_env,\n",
    "               learning_rate=1e-3,\n",
    "               policy=\"YOUR_POLICY\",\n",
    "               policy_kwargs={\"net_arch\": [100, 100, 100]}, # Just an example of architecture\n",
    "               n_steps=2,\n",
    "               batch_size=8,\n",
    "               verbose=True,\n",
    "               )\n",
    "```  \n",
    "and\n",
    "```python\n",
    "nn_model.learn(total_timesteps=LEARNING_ITERATION)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea5299",
   "metadata": {},
   "source": [
    "### Evaluate your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f21a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"saved_agent_DoNothingAgent\"\n",
    "path_save_results = \"{}_results\".format(save_path)\n",
    "shutil.rmtree(path_save_results, ignore_errors=True)\n",
    "\n",
    "\n",
    "runner = Runner(**env.get_params_for_runner(),\n",
    "                agentClass=my_agent\n",
    "               )\n",
    "res = runner.run(nb_episode=1, \n",
    "                 max_iter=max_iter,\n",
    "#                  pbar=tqdm,\n",
    "                 path_save=f\"./{path_save_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The results for DoNothing agent are:\")\n",
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a818cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path_save_results)\n",
    "EpisodeData.list_episode(path_save_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episodes = EpisodeData.list_episode(path_save_results)\n",
    "this_episode = EpisodeData.from_disk(*all_episodes[0])\n",
    "li_actions = this_episode.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a38328",
   "metadata": {},
   "source": [
    "Extraction of all the actions taken by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362aa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in li_actions:\n",
    "    dict_act_ = act.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_act_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc21815",
   "metadata": {},
   "source": [
    "We can see it is empty, which is normal because our agent is not taking any action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55dc1b",
   "metadata": {},
   "source": [
    "We can now check some observationnal values for the episode, here for example the status of the lines (connected/disconnected) at every step and count the number of real deconnections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_observations = this_episode.observations\n",
    "nb_real_disc = 0\n",
    "for obs_ in li_observations:\n",
    "    nb_real_disc += (obs_.line_status == False).sum()\n",
    "print(f'Total number of disconnected powerlines cumulated over all the timesteps : {nb_real_disc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5594d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_count = {}\n",
    "for act in li_actions:\n",
    "    act_as_vect = tuple(act.to_vect())\n",
    "    if not act_as_vect in actions_count:\n",
    "        actions_count[act_as_vect] = 0\n",
    "    actions_count[act_as_vect] += 1\n",
    "print(\"The agent did {} different valid actions:\\n\".format(len(actions_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52989ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in li_actions:\n",
    "    print(act)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
